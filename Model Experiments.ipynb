{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from APE import APE\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.linear_model import LinearRegression as LR\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from data_processing import data_processing\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('run_data_trained.csv')\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_train['Sweep Time']\n",
    "features = df_train.iloc[:,0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to confirm our suspisions that the current data set is sparse. To do this, multi linear regression coefficients will be analyzed. Large coefficients (relative to the other coefficients) is an indication that the matrix used in the OLS regression is close to a singular matrix, which happens when a variable(s) create rows in the matrix that are linearly dependent on the others. This occurs when there is little or no variance in a variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets visualize all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in features.columns:\n",
    "    plt.scatter(features[label],y,marker='o')\n",
    "    plt.title(label+' vs Sweep time')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that Nm, Np, Px, Py, and Pz contain no variance. As such, they will be removed from the data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = ['Nm','Np','Px','Py','Pz','Ag']\n",
    "features = features.drop(drop_list,axis=1)\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will look at the coefficients from a multi linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LR()\n",
    "model.fit(features,y)\n",
    "print('R^2')\n",
    "print(model.score(features,y))\n",
    "print('Slopes')\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we see that the coefficients associated with Ng and Ag are much greater than the others. This is evidence of sparsity in the data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(features)\n",
    "deltas = np.abs(predictions - y)\n",
    "mean = np.mean(deltas)*np.ones(y.shape[0])\n",
    "x = np.linspace(1,y.shape[0],y.shape[0])\n",
    "plt.semilogy(x,deltas,'o')\n",
    "plt.plot(x,mean)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some outliers that we fit very very well, byt there are more outliers that we are fitting poorly (the minority group that lies between 10^1 and 10^2). This error could be from a linear model not being good enough or from a sparse data set. If we try and fit an ANN, and get the same error distribution, then we will need to add data points to our set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = lambda x:  1/(1+np.exp(-x))\n",
    "feature_matrix,y,x_bar,x_std,y_bar,y_std = data_processing.input_generator('run_data_trained.csv')\n",
    "#val_struct = data_processing.input_generator('serial_val_data.csv')\n",
    "#val_x = val_struct[0]\n",
    "#val_y = val_struct[1]\n",
    "network = APE(sigmoid(feature_matrix),sigmoid(y),hidden_activation='elu',output_activation='sigmoid')\n",
    "grid = np.array([10,40,40,1])\n",
    "network.train(grid,200000,'time',loud=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix,y,x_bar,x_std,y_bar,y_std = data_processing.input_generator('run_data_trained.csv')\n",
    "output = network.predict(sigmoid(feature_matrix),'time')\n",
    "#un do the sigmoid activation function\n",
    "output = -np.log((1/output) - 1)\n",
    "#un normalize\n",
    "output = (output*y_std) + y_bar\n",
    "y = (y*y_std) + y_bar\n",
    "print(output)\n",
    "print(y)\n",
    "delta = np.abs(output.T - y)\n",
    "print(np.max(delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(1,y.shape[0],y.shape[0])\n",
    "mean = np.mean(delta)*np.ones(y.shape[0])\n",
    "print(mean)\n",
    "plt.semilogy(x,delta.T,'o',label='Absolute Error')\n",
    "plt.plot(x,mean,label='Average Absolute Error')\n",
    "plt.ylabel('Absolute Error (ns)')\n",
    "plt.xlabel('PDT run')\n",
    "plt.title('Absolute ANN Error')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "import csv\n",
    "with open('ann_error.csv','w') as writefile:\n",
    "    writer = csv.writer(writefile)\n",
    "    for i in (delta):\n",
    "        writer.writerow(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('test.csv')\n",
    "feature_matrix = np.zeros((383,6))\n",
    "y = np.array([])\n",
    "features = df.iloc[0:383].values\n",
    "for i in range(383):\n",
    "    feature_matrix[i][0] = features[i][4]\n",
    "    feature_matrix[i][1] = features[i][9]\n",
    "    feature_matrix[i][2] = features[i][10]\n",
    "    feature_matrix[i][3] = features[i][11]\n",
    "    feature_matrix[i][4] = features[i][12]\n",
    "    feature_matrix[i][5] = features[i][14]\n",
    "    y = np.append(y,features[i][17])\n",
    "x_bar = np.mean(feature_matrix,axis=0)\n",
    "x_std = np.std(feature_matrix,axis=0)\n",
    "y_bar = np.mean(y)\n",
    "y_std = np.std(y)\n",
    "#normalize\n",
    "feature_matrix = (feature_matrix-x_bar)/x_std\n",
    "y = (y - y_bar)/y_std\n",
    "val_x = feature_matrix[30]\n",
    "val_x = val_x.reshape(1,6)\n",
    "val_y = np.array([y[30]])\n",
    "feature_matrix = np.delete(feature_matrix,30,axis=0)\n",
    "y = np.delete(y,30)\n",
    "sigmoid = lambda x:  1/(1+np.exp(-x))\n",
    "network = APE(sigmoid(feature_matrix),sigmoid(y),val_x = sigmoid(val_x),val_y = sigmoid(val_y),hidden_activation='elu',output_activation='sigmoid',)\n",
    "grid = np.array([6,4,3,3,3,3,1]) \n",
    "histroy = network.train(grid,30000000,'time',loud=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = network.predict(sigmoid(feature_matrix),'time')\n",
    "#un do the sigmoid activation function\n",
    "output = -np.log((1/output) - 1)\n",
    "#un normalize\n",
    "output = (output*y_std) + y_bar\n",
    "y = (y*y_std) + y_bar\n",
    "delta = np.abs(output.T-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(1,y.shape[0],y.shape[0])\n",
    "mean = np.mean(delta)*np.ones(y.shape[0])\n",
    "print(np.max(delta))\n",
    "print(mean[0])\n",
    "plt.semilogy(x,delta.T,'o',label='Absolute Error')\n",
    "plt.plot(x,mean,label='Average Absolute Error')\n",
    "plt.ylabel('Absolute Error (ns)')\n",
    "plt.xlabel('PDT run')\n",
    "plt.title('Absolute ANN Error')\n",
    "plt.legend(loc='best')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "loss = np.array([histroy.history['loss']])\n",
    "val_loss = np.array([histroy.history['val_loss']])\n",
    "#build a quadratic for loss and val loss\n",
    "A = np.zeros((np.size(loss),3))\n",
    "x = np.linspace(1,np.size(loss),np.size(loss))\n",
    "for i in range(np.size(loss)):\n",
    "    A[i][0] = x[i]**2\n",
    "    A[i][1] = x[i]\n",
    "    A[i][2] = 1\n",
    "M = np.dot(A.T,A)\n",
    "b_l = np.dot(A.T,loss.T)\n",
    "b_vl = np.dot(A.T,val_loss.T)\n",
    "loss_coeff = np.linalg.solve(M,b_l)\n",
    "val_loss_coeff = np.linalg.solve(M,b_vl)\n",
    "loss_func = lambda x:loss_coeff[0]*x**2 + loss_coeff[1]*x + loss_coeff[2]\n",
    "val_loss_func = lambda x:val_loss_coeff[0]*x**2 + val_loss_coeff[1]*x + val_loss_coeff[2]\n",
    "#plt.plot(x,val_loss.T,'o',label='val_loss')\n",
    "#plt.semilogy(x,loss.T,label='loss')\n",
    "plt.plot(x,val_loss_func(x),label='val_loss quad fit')\n",
    "plt.plot(x,loss_func(x),label='loss quad fit')\n",
    "plt.legend(loc='best')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
